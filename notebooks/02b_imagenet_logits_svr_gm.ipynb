{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a36aa7-0d94-47c9-8baf-748c27b4418c",
   "metadata": {},
   "source": [
    "### Setup (paths, folds, model lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65eec65f-26e4-4c44-a818-42a900e1a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>fold0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  fold0  \n",
       "0          0      1        0      0          0     0     0           63      0  \n",
       "1          0      0        0      0          0     0     0           42      7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, gc, time\n",
    "from PIL import Image\n",
    "import torch, timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "ROOT = Path(\"/workspace/pet-finder\")\n",
    "DATA = ROOT/\"data\"/\"raw\"\n",
    "PROC = ROOT/\"data\"/\"processed\"; PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_df = pd.read_csv(DATA/\"train.csv\")\n",
    "test_ids = sorted(p.stem for p in (DATA/\"test\").glob(\"*.jpg\"))\n",
    "train_ids = train_df[\"Id\"].astype(str).tolist()\n",
    "\n",
    "# 20 stratified folds on binned target (exactly like GM)\n",
    "train_df[\"bins\"] = (train_df[\"Pawpularity\"]//5).round()\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=20, shuffle=True, random_state=1)\n",
    "fold_col = \"fold0\"; train_df[fold_col] = -1\n",
    "for i, (_, val_idx) in enumerate(skf.split(train_df.index, train_df[\"bins\"])):\n",
    "    train_df.loc[val_idx, fold_col] = i\n",
    "train_df[fold_col] = train_df[fold_col].astype(int)\n",
    "train_df.drop(columns=[\"bins\"], inplace=True)\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcefa9-53c3-48b3-a4d2-02896e01e849",
   "metadata": {},
   "source": [
    "#### Backbone lists (same idea/names as GM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2cf7037-1d1d-4f95-977b-b23a79a5af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clip_RN101',\n",
       " 'clip_RN50',\n",
       " 'clip_RN50x16',\n",
       " 'clip_RN50x4',\n",
       " 'clip_ViT-B-16',\n",
       " 'clip_ViT-B-32',\n",
       " 'deit_base_distilled_patch16_384',\n",
       " 'deit_base_distilled_patch16_384_hflip_384',\n",
       " 'fbnetc_100',\n",
       " 'ig_resnext101_32x48d',\n",
       " 'ig_resnext101_32x48d_hflip_384',\n",
       " 'ig_resnext101_32x8d',\n",
       " 'repvgg_b0',\n",
       " 'resnest269e',\n",
       " 'resnetv2_152x4_bitm',\n",
       " 'rexnet_200',\n",
       " 'swsl_resnext101_32x8d',\n",
       " 'tf_efficientnet_b6_ns',\n",
       " 'tf_efficientnet_b7_ns',\n",
       " 'tf_efficientnet_b8_ap',\n",
       " 'tf_efficientnet_l2_ns_475',\n",
       " 'tf_efficientnet_l2_ns_512',\n",
       " 'tf_efficientnet_l2_ns_hflip_384',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_r50_s32_384']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main backbones producing 1000-d logits from timm\n",
    "names_main = [\n",
    "    'deit_base_distilled_patch16_384',\n",
    "    'fbnetc_100',\n",
    "    'ig_resnext101_32x8d',\n",
    "    'ig_resnext101_32x48d',\n",
    "    'repvgg_b0',\n",
    "    'resnetv2_152x4_bitm',\n",
    "    'rexnet_200',\n",
    "    'resnest269e',\n",
    "    'swsl_resnext101_32x8d',\n",
    "    'tf_efficientnet_b6_ns',\n",
    "    'tf_efficientnet_b7_ns',\n",
    "    'tf_efficientnet_b8_ap',\n",
    "    'tf_efficientnet_l2_ns_475',\n",
    "    'vit_base_patch16_384',\n",
    "    'vit_large_patch16_384',\n",
    "    'vit_large_r50_s32_384',\n",
    "]\n",
    "\n",
    "# Variants with HFlip or size override encoded in the name (weâ€™ll parse them)\n",
    "names_hflip_crop = [\n",
    "    'tf_efficientnet_l2_ns_hflip_384',\n",
    "    'deit_base_distilled_patch16_384_hflip_384',\n",
    "    'ig_resnext101_32x48d_hflip_384',\n",
    "    'tf_efficientnet_l2_ns_512',           # same backbone, bigger input\n",
    "]\n",
    "\n",
    "# CLIP family (features from encode_image, not 1k logits)\n",
    "clip_names = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B-16', 'ViT-B-32']\n",
    "\n",
    "# Subsets used for SVR A / B / C (as in GM)\n",
    "names0 = [\n",
    "    'clip_RN50x16','clip_ViT-B-32','clip_ViT-B-16','clip_RN50x4',\n",
    "    'deit_base_distilled_patch16_384','ig_resnext101_32x48d',\n",
    "    'repvgg_b0','resnetv2_152x4_bitm','swsl_resnext101_32x8d',\n",
    "    'tf_efficientnet_l2_ns_475','vit_base_patch16_384','vit_large_r50_s32_384'\n",
    "]\n",
    "names1 = [\n",
    "    'clip_RN50x16','clip_RN101','clip_RN50','fbnetc_100',\n",
    "    'ig_resnext101_32x8d','rexnet_200','resnest269e',\n",
    "    'tf_efficientnet_b6_ns','tf_efficientnet_b8_ap','tf_efficientnet_b7_ns',\n",
    "    'vit_large_patch16_384'\n",
    "]\n",
    "names2 = [\n",
    "    'tf_efficientnet_l2_ns_hflip_384','deit_base_distilled_patch16_384_hflip_384',\n",
    "    'ig_resnext101_32x48d_hflip_384','tf_efficientnet_l2_ns_512',\n",
    "    'ig_resnext101_32x48d','vit_large_r50_s32_384',\n",
    "    'clip_RN50x4','clip_ViT-B-16','clip_RN50x16','clip_ViT-B-32'\n",
    "]\n",
    "\n",
    "names_all = sorted(set(names_main + names_hflip_crop + [f\"clip_{m}\" for m in clip_names]))\n",
    "names_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf23417-546c-4883-8bac-aaa70a388917",
   "metadata": {},
   "source": [
    "### Feature factory (timm logits + hflip/size variants + CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dadeec6-3ee2-4230-98b6-7e284353969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Feature-factory helpers (timm logits + hflip/size variants + CLIP) =====\n",
    "from timm.data import create_transform\n",
    "try:\n",
    "    from timm.data import resolve_data_config\n",
    "except Exception:\n",
    "    from timm.data import resolve_model_data_config as resolve_data_config\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _make_transform(model, img_size=None, crop_pct=None, is_train=False):\n",
    "    cfg = resolve_data_config({}, model=model)\n",
    "    if img_size is not None:\n",
    "        cfg[\"input_size\"] = (3, int(img_size), int(img_size))\n",
    "    if crop_pct is not None:\n",
    "        cfg[\"crop_pct\"] = float(crop_pct)\n",
    "    cfg[\"is_training\"] = bool(is_train)\n",
    "    return create_transform(**cfg)\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, ids, split, tfm, hflip=False, crop_border=False):\n",
    "        self.ids, self.split, self.tfm = ids, split, tfm\n",
    "        self.hflip, self.crop_border = hflip, crop_border\n",
    "        self.root = DATA / split\n",
    "    def __len__(self): return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.root / f\"{self.ids[i]}.jpg\").convert(\"RGB\")\n",
    "        if self.hflip:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if self.crop_border:\n",
    "            w, h = img.size\n",
    "            img = img.crop((0, int(0.02*h), int(0.98*w), int(0.98*h)))\n",
    "        return self.tfm(img)\n",
    "\n",
    "def _batch_size_for(name):\n",
    "    # Slightly smaller for big EfficientNet-L2 variants\n",
    "    return 10 if \"tf_efficientnet_l2_ns\" in name else 16\n",
    "\n",
    "def _cache_path(name, split):\n",
    "    return PROC / f\"{name}_{split}.npy\"\n",
    "\n",
    "# ---------- timm backbones (1000-d logits) ----------\n",
    "def extract_timm(name, split):\n",
    "    \"\"\"\n",
    "    Supports modifiers:\n",
    "      - '<backbone>_hflip_<size>' -> horizontal flip + slight crop at <size>\n",
    "      - '<backbone>_512'          -> size override to 512 (no flip)\n",
    "    \"\"\"\n",
    "    hflip = \"_hflip_\" in name\n",
    "    size_override = None\n",
    "    base = name\n",
    "    if hflip:\n",
    "        base, last = name.split(\"_hflip_\")\n",
    "        size_override = int(last)\n",
    "    elif name.endswith(\"_512\"):  # intentionally narrow to avoid matching real model names like *_475\n",
    "        base, size_override = name.rsplit(\"_\", 1)\n",
    "        size_override = int(size_override)\n",
    "\n",
    "    out = _cache_path(name, split)\n",
    "    if out.exists():\n",
    "        return np.load(out)\n",
    "\n",
    "    model = timm.create_model(base, pretrained=True).eval().to(device)\n",
    "    tfm = _make_transform(model, img_size=size_override, crop_pct=1.0 if hflip else None, is_train=False)\n",
    "\n",
    "    ids = train_ids if split == \"train\" else test_ids\n",
    "    ds = ImgDataset(ids, \"train\" if split == \"train\" else \"test\", tfm=tfm, hflip=hflip, crop_border=hflip)\n",
    "    dl = DataLoader(ds, batch_size=_batch_size_for(base), shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    feats = []\n",
    "    torch.set_grad_enabled(False)\n",
    "    for xb in dl:\n",
    "        xb = xb.to(device)\n",
    "        with torch.amp.autocast(\"cuda\", enabled=(device == \"cuda\")):\n",
    "            y = model(xb)\n",
    "        feats.append(y.float().cpu().numpy())\n",
    "    arr = np.concatenate(feats, 0).astype(\"float32\")\n",
    "    np.save(out, arr)\n",
    "    return arr\n",
    "\n",
    "# --- CLIP name mapping (hyphen -> slash) ---\n",
    "CLIP_NAME_MAP = {\n",
    "    \"ViT-B-16\": \"ViT-B/16\",\n",
    "    \"ViT-B-32\": \"ViT-B/32\",\n",
    "    \"ViT-L-14\": \"ViT-L/14\",\n",
    "    \"ViT-L-14@336px\": \"ViT-L/14@336px\",\n",
    "    \"RN50\": \"RN50\",\n",
    "    \"RN101\": \"RN101\",\n",
    "    \"RN50x4\": \"RN50x4\",\n",
    "    \"RN50x16\": \"RN50x16\",\n",
    "    \"RN50x64\": \"RN50x64\",\n",
    "}\n",
    "def _clip_canonical(name: str) -> str:\n",
    "    return CLIP_NAME_MAP.get(name, name)\n",
    "\n",
    "# ---------- CLIP features ----------\n",
    "def extract_clip(clip_name, split):\n",
    "    \"\"\"\n",
    "    Extract CLIP image embeddings using openai/CLIP.\n",
    "    Cache filenames keep hyphen style (e.g., clip_ViT-B-16_*), while clip.load uses slash form.\n",
    "    \"\"\"\n",
    "    cache_name = f\"clip_{clip_name}\"\n",
    "    out = _cache_path(cache_name, split)\n",
    "    if out.exists():\n",
    "        return np.load(out)\n",
    "\n",
    "    import clip\n",
    "    cn = _clip_canonical(clip_name)  # convert to slash form for clip.load\n",
    "    try:\n",
    "        model, preprocess = clip.load(cn, device=device)\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"CLIP load failed for '{clip_name}' (canonical '{cn}'). \"\n",
    "            f\"Available: {clip.available_models()}\"\n",
    "        ) from e\n",
    "    model.eval()\n",
    "\n",
    "    class ClipDS(Dataset):\n",
    "        def __init__(self, ids, split, preprocess):\n",
    "            self.ids, self.split, self.pp = ids, split, preprocess\n",
    "        def __len__(self): return len(self.ids)\n",
    "        def __getitem__(self, i):\n",
    "            img = Image.open(DATA / self.split / f\"{self.ids[i]}.jpg\").convert(\"RGB\")\n",
    "            return self.pp(img)\n",
    "\n",
    "    ids = train_ids if split == \"train\" else test_ids\n",
    "    dl = DataLoader(ClipDS(ids, \"train\" if split == \"train\" else \"test\", preprocess),\n",
    "                    batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for xb in dl:\n",
    "            xb = xb.to(device)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(device == \"cuda\")):\n",
    "                z = model.encode_image(xb)\n",
    "            feats.append(z.float().cpu().numpy())\n",
    "    arr = np.concatenate(feats, 0).astype(\"float32\")\n",
    "    np.save(out, arr)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468b949-0225-484a-a4f2-25a545259bde",
   "metadata": {},
   "source": [
    "#### Run extraction (it will cache to data/processed/*.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e5c5f6-1f6f-4506-956d-f364df326518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit_base_distilled_patch16_384           train (9912, 1000)  test (8, 1000)  | 0s\n",
      "fbnetc_100                                train (9912, 1000)  test (8, 1000)  | 0s\n",
      "ig_resnext101_32x8d                       train (9912, 1000)  test (8, 1000)  | 0s\n",
      "ig_resnext101_32x48d                      train (9912, 1000)  test (8, 1000)  | 0s\n",
      "repvgg_b0                                 train (9912, 1000)  test (8, 1000)  | 0s\n",
      "resnetv2_152x4_bitm                       train (9912, 1000)  test (8, 1000)  | 0s\n",
      "rexnet_200                                train (9912, 1000)  test (8, 1000)  | 0s\n",
      "resnest269e                               train (9912, 1000)  test (8, 1000)  | 0s\n",
      "swsl_resnext101_32x8d                     train (9912, 1000)  test (8, 1000)  | 0s\n",
      "tf_efficientnet_b6_ns                     train (9912, 1000)  test (8, 1000)  | 0s\n",
      "tf_efficientnet_b7_ns                     train (9912, 1000)  test (8, 1000)  | 0s\n",
      "tf_efficientnet_b8_ap                     train (9912, 1000)  test (8, 1000)  | 0s\n",
      "tf_efficientnet_l2_ns_475                 train (9912, 1000)  test (8, 1000)  | 0s\n",
      "vit_base_patch16_384                      train (9912, 1000)  test (8, 1000)  | 0s\n",
      "vit_large_patch16_384                     train (9912, 1000)  test (8, 1000)  | 0s\n",
      "vit_large_r50_s32_384                     train (9912, 1000)  test (8, 1000)  | 0s\n",
      "tf_efficientnet_l2_ns_hflip_384           train (9912, 1000)  test (8, 1000)  | 0s\n",
      "deit_base_distilled_patch16_384_hflip_384  train (9912, 1000)  test (8, 1000)  | 0s\n",
      "ig_resnext101_32x48d_hflip_384            train (9912, 1000)  test (8, 1000)  | 0s\n",
      "tf_efficientnet_l2_ns_512                 train (9912, 1000)  test (8, 1000)  | 0s\n",
      "clip_RN50                                 train (9912, 1024)  test (8, 1024)  | 0s\n",
      "clip_RN101                                train (9912, 512)  test (8, 512)  | 0s\n",
      "clip_RN50x4                               train (9912, 640)  test (8, 640)  | 0s\n",
      "clip_RN50x16                              train (9912, 768)  test (8, 768)  | 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:08<00:00, 40.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_ViT-B-16                             train (9912, 512)  test (8, 512)  | 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:03<00:00, 103MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_ViT-B-32                             train (9912, 512)  test (8, 512)  | 17s\n"
     ]
    }
   ],
   "source": [
    "to_run = names_main + names_hflip_crop + [f\"clip_{m}\" for m in clip_names]\n",
    "\n",
    "EMB_TRAIN, EMB_TEST = {}, {}\n",
    "for name in to_run:\n",
    "    t0 = time.time()\n",
    "    if name.startswith(\"clip_\"):\n",
    "        m = name.split(\"clip_\")[1]\n",
    "        tr = extract_clip(m, \"train\")\n",
    "        te = extract_clip(m, \"test\")\n",
    "    else:\n",
    "        tr = extract_timm(name, \"train\")\n",
    "        te = extract_timm(name, \"test\")\n",
    "    EMB_TRAIN[name] = tr\n",
    "    EMB_TEST[name]  = te\n",
    "    print(f\"{name:40s}  train {tr.shape}  test {te.shape}  | {int(time.time()-t0)}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74cbab-4440-4f5a-844f-2238f6730498",
   "metadata": {},
   "source": [
    "### Train three cuML SVRs (A/B/C) on concatenated stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3831f5bc-3d97-42ca-b8b2-2bfa596e62a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR A: using 12 feature groups; Xtr=(9912, 10432), Xte=(8, 10432)\n",
      "   A RMSE: 17.15738\n",
      "SVR B: using 11 feature groups; Xtr=(9912, 10304), Xte=(8, 10304)\n",
      "   B RMSE: 17.16733\n",
      "SVR C: using 10 feature groups; Xtr=(9912, 8432), Xte=(8, 8432)\n",
      "   C RMSE: 17.05627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np, gc\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "def build_stack_noscale(keys):\n",
    "    # Use only keys that exist in both train & test dicts\n",
    "    keys = [k for k in keys if (k in EMB_TRAIN) and (k in EMB_TEST)]\n",
    "    Xtr = np.concatenate([EMB_TRAIN[k] for k in keys], axis=1).astype(\"float32\")\n",
    "    Xte = np.concatenate([EMB_TEST[k]  for k in keys], axis=1).astype(\"float32\")\n",
    "    # Clean any odd values up front\n",
    "    Xtr = np.nan_to_num(Xtr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    Xte = np.nan_to_num(Xte, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return Xtr, Xte, keys\n",
    "\n",
    "def fit_svr_perfold(TRAIN, TEST, kfold_col=\"fold0\", prefer_cuml=True):\n",
    "    y = train_df[\"Pawpularity\"].values.astype(\"float32\")\n",
    "    folds = int(train_df[kfold_col].max()) + 1\n",
    "    oof   = np.full(len(y), np.nan, dtype=\"float32\")\n",
    "    ytest = np.zeros(len(TEST), dtype=\"float32\")\n",
    "\n",
    "    # Decide backend once\n",
    "    use_cuml = prefer_cuml\n",
    "    try:\n",
    "        from cuml.svm import SVR as cuSVR\n",
    "    except Exception:\n",
    "        use_cuml = False\n",
    "\n",
    "    for fold in range(folds):\n",
    "        tr_idx = train_df[kfold_col] != fold\n",
    "        va_idx = train_df[kfold_col] == fold\n",
    "\n",
    "        # Per-fold standardization (avoids zero-variance / leakage issues)\n",
    "        mu = TRAIN[tr_idx].mean(0)\n",
    "        sd = TRAIN[tr_idx].std(0)\n",
    "        sd[sd < 1e-6] = 1.0\n",
    "\n",
    "        Xtr = ((TRAIN[tr_idx] - mu) / sd).astype(\"float32\")\n",
    "        Xva = ((TRAIN[va_idx] - mu) / sd).astype(\"float32\")\n",
    "        Xte = ((TEST        - mu) / sd).astype(\"float32\")\n",
    "\n",
    "        if use_cuml:\n",
    "            model = cuSVR(kernel=\"rbf\", C=16.0, epsilon=0.1, max_iter=4000, output_type=\"numpy\")\n",
    "        else:\n",
    "            from sklearn.svm import SVR as skSVR\n",
    "            from sklearn.pipeline import make_pipeline\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            model = make_pipeline(\n",
    "                StandardScaler(with_mean=True, with_std=True),\n",
    "                skSVR(kernel=\"rbf\", C=10.0, epsilon=0.1, gamma=\"scale\", max_iter=4000),\n",
    "            )\n",
    "\n",
    "        model.fit(Xtr, np.clip(y[tr_idx], 1, 85))\n",
    "        pred_va = np.asarray(model.predict(Xva), dtype=\"float32\")\n",
    "        pred_te = np.asarray(model.predict(Xte), dtype=\"float32\")\n",
    "\n",
    "        oof[va_idx] = np.clip(pred_va, 1, 100)\n",
    "        ytest += np.clip(pred_te, 1, 100) / folds\n",
    "\n",
    "        del model; gc.collect()\n",
    "\n",
    "    # Final cleanup to guarantee finiteness\n",
    "    oof   = np.nan_to_num(oof,   nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    ytest = np.nan_to_num(ytest, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return oof, ytest\n",
    "\n",
    "def run_stack(label, keys):\n",
    "    Xtr, Xte, used = build_stack_noscale(keys)\n",
    "    print(f\"SVR {label}: using {len(used)} feature groups; Xtr={Xtr.shape}, Xte={Xte.shape}\")\n",
    "\n",
    "    # Try cuML first\n",
    "    oof, te = fit_svr_perfold(Xtr, Xte, fold_col, prefer_cuml=True)\n",
    "    r = rmse(train_df[\"Pawpularity\"].values, oof)\n",
    "    print(f\"   {label} RMSE: {r:.5f}\")\n",
    "\n",
    "    # If NaN, retry with sklearn (robust fallback)\n",
    "    if not np.isfinite(r):\n",
    "        print(f\"   {label}: NaN detected â€” retrying with scikit-learn SVR\")\n",
    "        oof, te = fit_svr_perfold(Xtr, Xte, fold_col, prefer_cuml=False)\n",
    "        r = rmse(train_df[\"Pawpularity\"].values, oof)\n",
    "        print(f\"   {label} RMSE (sklearn): {r:.5f}\")\n",
    "\n",
    "    return oof, te\n",
    "\n",
    "# ---- Run A / B / C ----\n",
    "oofA, teA = run_stack(\"A\", names0)\n",
    "oofB, teB = run_stack(\"B\", names1)\n",
    "oofC, teC = run_stack(\"C\", names2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33742273-ed24-41c4-a5b2-640687b7fa47",
   "metadata": {},
   "source": [
    "### Blend A/B/C using OOF (Nelderâ€“Mead) and make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e14f95b1-e3eb-4557-8dff-1f6ce1dbed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend weights: [0.09887976 0.30275614 0.63030819] | OOF RMSE: 16.98492864442947\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
       "      <td>39.748593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43a2262d7738e3d420d453815151079e</td>\n",
       "      <td>39.844164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
       "      <td>39.779759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
       "      <td>39.598422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
       "      <td>39.741290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Pawpularity\n",
       "0  4128bae22183829d2b5fea10effdb0c3    39.748593\n",
       "1  43a2262d7738e3d420d453815151079e    39.844164\n",
       "2  4e429cead1848a298432a0acad014c9d    39.779759\n",
       "3  80bc3ccafcc51b66303c2c263aa38486    39.598422\n",
       "4  8f49844c382931444e68dffbe20228f4    39.741290"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize weights on OOF to minimize RMSE (no leakage)\n",
    "oofA, oofB, oofC = [oofA.astype(\"float32\"), oofB.astype(\"float32\"), oofC.astype(\"float32\")]\n",
    "teA,  teB,  teC  = [teA.astype(\"float32\"),  teB.astype(\"float32\"),  teC.astype(\"float32\")]\n",
    "\n",
    "try:\n",
    "    from scipy.optimize import minimize\n",
    "    def objective(K):\n",
    "        yhat = K[0]*oofA + K[1]*oofB + K[2]*oofC\n",
    "        return rmse(train_df[\"Pawpularity\"].values, yhat)\n",
    "    res = minimize(objective, [1/3]*3, method=\"Nelder-Mead\", tol=1e-6)\n",
    "    K = res.x\n",
    "    print(\"Blend weights:\", K, \"| OOF RMSE:\", res.fun)\n",
    "except Exception:\n",
    "    # fallback: tiny ridge on OOF\n",
    "    from sklearn.linear_model import Ridge\n",
    "    X = np.vstack([oofA, oofB, oofC]).T\n",
    "    y = train_df[\"Pawpularity\"].values.astype(\"float32\")\n",
    "    w = Ridge(alpha=1e-3, fit_intercept=False).fit(X, y).coef_\n",
    "    K = w / w.sum()\n",
    "    print(\"Blend weights (ridge):\", K, \"| OOF RMSE:\", rmse(y, X @ K))\n",
    "\n",
    "# Blend test\n",
    "test_pred = K[0]*teA + K[1]*teB + K[2]*teC\n",
    "\n",
    "# GMâ€™s small calibration\n",
    "test_pred = np.clip(1.032 * test_pred, 0, 100)\n",
    "\n",
    "sub = pd.DataFrame({\"Id\": test_ids, \"Pawpularity\": test_pred})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
